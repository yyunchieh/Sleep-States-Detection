{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas.api.types\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_data = pd.read_csv(\"individual_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "hidden_dim = 64\n",
    "num_layers = 3\n",
    "num_classes = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TimeseriesDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(individual_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m      4\u001b[0m train_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.8\u001b[39m) \n\u001b[1;32m----> 6\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTimeseriesDataset\u001b[49m(X\u001b[38;5;241m=\u001b[39mX[train_idx:], y\u001b[38;5;241m=\u001b[39my[train_idx:], seq_len\u001b[38;5;241m=\u001b[39mseq_len, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m DataLoader(valid_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TimeseriesDataset' is not defined"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(individual_data[['anglez', 'enmo', 'anglez_change', 'enmo_change']].values, dtype=torch.float32)\n",
    "y = torch.tensor(individual_data['event'].values, dtype=torch.int64)\n",
    "\n",
    "train_idx = int(len(X) * 0.8) \n",
    "\n",
    "valid_dataset = TimeseriesDataset(X=X[train_idx:], y=y[train_idx:], seq_len=seq_len, transform=None)\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\4019-tjyen\\AppData\\Local\\Temp\\ipykernel_37416\\3968737724.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(4, 64, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_dim).to(device)\n",
    "        c0 = torch.zeros(num_layers, x.size(0), hidden_dim).to(device)\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=num_classes).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      2\u001b[0m all_confidence_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_confidence_scores = []\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in valid_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        \n",
    "        outputs = model(X_batch)\n",
    "        confidence_scores = calculate_confidence_score(outputs)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_confidence_scores.extend(confidence_scores.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Set some placeholders for global parameters\n",
    "series_id_column_name = \"series_id\"\n",
    "time_column_name = \"time\"\n",
    "event_column_name = \"event\"\n",
    "#score?\n",
    "score_column_name = \"score\"\n",
    "use_scoring_intervals = None\n",
    "\n",
    "\n",
    "def score(\n",
    "        solution: pd.individual_data,\n",
    "        #submussion dataframe?\n",
    "        submission: pd.DataFrame,\n",
    "        tolerances: Dict[str, List[float]],\n",
    "        series_id_column_name: \"series_id\",\n",
    "        time_column_name: \"time\",\n",
    "        event_column_name: \"event\",\n",
    "        score_column_name: \"score\",\n",
    "        use_scoring_intervals: bool = False,\n",
    ") -> float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Validate metric parameters\n",
    "    assert len(tolerances) > 0, \"Events must have defined tolerances.\"\n",
    "    assert set(tolerances.keys()) == set(solution[event_column_name]).difference({'start', 'end'}),\\\n",
    "        (f\"Solution column {event_column_name} must contain the same events \"\n",
    "         \"as defined in tolerances.\")\n",
    "    assert pd.api.types.is_numeric_dtype(solution[time_column_name]),\\\n",
    "        f\"Solution column {time_column_name} must be of numeric type.\"\n",
    "\n",
    "    # Validate submission format\n",
    "    for column_name in [\n",
    "        series_id_column_name,\n",
    "        time_column_name,\n",
    "        event_column_name,\n",
    "        score_column_name,\n",
    "    ]:\n",
    "        if column_name not in submission.columns:\n",
    "            raise ParticipantVisibleError(f\"Submission must have column '{target_name}'.\")\n",
    "\n",
    "    if not pd.api.types.is_numeric_dtype(submission[time_column_name]):\n",
    "        raise ParticipantVisibleError(\n",
    "            f\"Submission column '{time_column_name}' must be of numeric type.\"\n",
    "        )\n",
    "    if not pd.api.types.is_numeric_dtype(submission[score_column_name]):\n",
    "        raise ParticipantVisibleError(\n",
    "            f\"Submission column '{score_column_name}' must be of numeric type.\"\n",
    "        )\n",
    "\n",
    "# Set these globally to avoid passing around a bunch of arguments\n",
    "globals()['series_id_column_name'] = series_id_column_name\n",
    "globals()['time_column_name'] = time_column_name\n",
    "globals()['event_column_name'] = event_column_name\n",
    "globals()['score_column_name'] = score_column_name\n",
    "globals()['use_scoring_intervals'] = use_scoring_intervals\n",
    "\n",
    "    return event_detection_ap(solution, submission, tolerances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_detections(\n",
    "        detections: pd.DataFrame, intervals: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Drop detections not inside a scoring interval.\"\"\"\n",
    "    detection_time = detections.loc[:, time_column_name].sort_values().to_numpy()\n",
    "    intervals = intervals.to_numpy()\n",
    "    is_scored = np.full_like(detection_time, False, dtype=bool)\n",
    "\n",
    "    i, j = 0, 0\n",
    "    while i < len(detection_time) and j < len(intervals):\n",
    "        time = detection_time[i]\n",
    "        int_ = intervals[j]\n",
    "\n",
    "        # If the detection is prior in time to the interval, go to the next detection.\n",
    "        if time < int_.left:\n",
    "            i += 1\n",
    "        # If the detection is inside the interval, keep it and go to the next detection.\n",
    "        elif time in int_:\n",
    "            is_scored[i] = True\n",
    "            i += 1\n",
    "        # If the detection is later in time, go to the next interval.\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    return detections.loc[is_scored].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def match_detections(\n",
    "        tolerance: float, ground_truths: pd.DataFrame, detections: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Match detections to ground truth events. Arguments are taken from a common event x tolerance x series_id evaluation group.\"\"\"\n",
    "    detections_sorted = detections.sort_values(score_column_name, ascending=False).dropna()\n",
    "    is_matched = np.full_like(detections_sorted[event_column_name], False, dtype=bool)\n",
    "    gts_matched = set()\n",
    "    for i, det in enumerate(detections_sorted.itertuples(index=False)):\n",
    "        best_error = tolerance\n",
    "        best_gt = None\n",
    "\n",
    "        for gt in ground_truths.itertuples(index=False):\n",
    "            error = abs(getattr(det, time_column_name) - getattr(gt, time_column_name))\n",
    "            if error < best_error and gt not in gts_matched:\n",
    "                best_gt = gt\n",
    "                best_error = error\n",
    "\n",
    "        if best_gt is not None:\n",
    "            is_matched[i] = True\n",
    "            gts_matched.add(best_gt)\n",
    "\n",
    "    detections_sorted['matched'] = is_matched\n",
    "\n",
    "    return detections_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve(\n",
    "        matches: np.ndarray, scores: np.ndarray, p: int\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    if len(matches) == 0:\n",
    "        return [1], [0], []\n",
    "\n",
    "    # Sort matches by decreasing confidence\n",
    "    idxs = np.argsort(scores, kind='stable')[::-1]\n",
    "    scores = scores[idxs]\n",
    "    matches = matches[idxs]\n",
    "\n",
    "    distinct_value_indices = np.where(np.diff(scores))[0]\n",
    "    threshold_idxs = np.r_[distinct_value_indices, matches.size - 1]\n",
    "    thresholds = scores[threshold_idxs]\n",
    "\n",
    "    # Matches become TPs and non-matches FPs as confidence threshold decreases\n",
    "    tps = np.cumsum(matches)[threshold_idxs]\n",
    "    fps = np.cumsum(~matches)[threshold_idxs]\n",
    "\n",
    "    precision = tps / (tps + fps)\n",
    "    precision[np.isnan(precision)] = 0\n",
    "    recall = tps / p  # total number of ground truths might be different than total number of matches\n",
    "\n",
    "    # Stop when full recall attained and reverse the outputs so recall is non-increasing.\n",
    "    last_ind = tps.searchsorted(tps[-1])\n",
    "    sl = slice(last_ind, None, -1)\n",
    "\n",
    "    # Final precision is 1 and final recall is 0\n",
    "    return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]\n",
    "\n",
    "\n",
    "def average_precision_score(matches: np.ndarray, scores: np.ndarray, p: int) -> float:\n",
    "    precision, recall, _ = precision_recall_curve(matches, scores, p)\n",
    "    # Compute step integral\n",
    "    return -np.sum(np.diff(recall) * np.array(precision)[:-1])\n",
    "\n",
    "\n",
    "def event_detection_ap(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        tolerances: Dict[str, List[float]],\n",
    ") -> float:\n",
    "\n",
    "    # Ensure solution and submission are sorted properly\n",
    "    solution = solution.sort_values([series_id_column_name, time_column_name])\n",
    "    submission = submission.sort_values([series_id_column_name, time_column_name])\n",
    "\n",
    "    # Extract scoring intervals.\n",
    "    if use_scoring_intervals:\n",
    "        intervals = (\n",
    "            solution\n",
    "            .query(\"event in ['start', 'end']\")\n",
    "            .assign(interval=lambda x: x.groupby([series_id_column_name, event_column_name]).cumcount())\n",
    "            .pivot(\n",
    "                index='interval',\n",
    "                columns=[series_id_column_name, event_column_name],\n",
    "                values=time_column_name,\n",
    "            )\n",
    "            .stack(series_id_column_name)\n",
    "            .swaplevel()\n",
    "            .sort_index()\n",
    "            .loc[:, ['start', 'end']]\n",
    "            .apply(lambda x: pd.Interval(*x, closed='both'), axis=1)\n",
    "        )\n",
    "\n",
    "    # Extract ground-truth events.\n",
    "    ground_truths = (\n",
    "        solution\n",
    "        .query(\"event not in ['start', 'end']\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Map each event class to its prevalence (needed for recall calculation)\n",
    "    class_counts = ground_truths.value_counts(event_column_name).to_dict()\n",
    "\n",
    "    # Create table for detections with a column indicating a match to a ground-truth event\n",
    "    detections = submission.assign(matched = False)\n",
    "\n",
    "    # Remove detections outside of scoring intervals\n",
    "    if use_scoring_intervals:\n",
    "        detections_filtered = []\n",
    "        for (det_group, dets), (int_group, ints) in zip(\n",
    "            detections.groupby(series_id_column_name), intervals.groupby(series_id_column_name)\n",
    "        ):\n",
    "            assert det_group == int_group\n",
    "            detections_filtered.append(filter_detections(dets, ints))\n",
    "        detections_filtered = pd.concat(detections_filtered, ignore_index=True)\n",
    "    else:\n",
    "        detections_filtered = detections\n",
    "\n",
    "    # Create table of event-class x tolerance x series_id values\n",
    "    aggregation_keys = pd.DataFrame(\n",
    "        [(ev, tol, vid)\n",
    "         for ev in tolerances.keys()\n",
    "         for tol in tolerances[ev]\n",
    "         for vid in ground_truths[series_id_column_name].unique()],\n",
    "        columns=[event_column_name, 'tolerance', series_id_column_name],\n",
    "    )\n",
    "\n",
    "    # Create match evaluation groups: event-class x tolerance x series_id\n",
    "    detections_grouped = (\n",
    "        aggregation_keys\n",
    "        .merge(detections_filtered, on=[event_column_name, series_id_column_name], how='left')\n",
    "        .groupby([event_column_name, 'tolerance', series_id_column_name])\n",
    "    )\n",
    "    ground_truths_grouped = (\n",
    "        aggregation_keys\n",
    "        .merge(ground_truths, on=[event_column_name, series_id_column_name], how='left')\n",
    "        .groupby([event_column_name, 'tolerance', series_id_column_name])\n",
    "    )\n",
    "    # Match detections to ground truth events by evaluation group\n",
    "    detections_matched = []\n",
    "    for key in aggregation_keys.itertuples(index=False):\n",
    "        dets = detections_grouped.get_group(key)\n",
    "        gts = ground_truths_grouped.get_group(key)\n",
    "        detections_matched.append(\n",
    "            match_detections(dets['tolerance'].iloc[0], gts, dets)\n",
    "        )\n",
    "    detections_matched = pd.concat(detections_matched)\n",
    "\n",
    "    # Compute AP per event x tolerance group\n",
    "    event_classes = ground_truths[event_column_name].unique()\n",
    "    ap_table = (\n",
    "        detections_matched\n",
    "        .query(\"event in @event_classes\")\n",
    "        .groupby([event_column_name, 'tolerance']).apply(\n",
    "            lambda group: average_precision_score(\n",
    "                group['matched'].to_numpy(),\n",
    "                group[score_column_name].to_numpy(),\n",
    "                class_counts[group[event_column_name].iat[0]],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # Average over tolerances, then over event classes\n",
    "    mean_ap = ap_table.groupby(event_column_name).mean().sum() / len(event_classes)\n",
    "\n",
    "    return mean_ap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umap_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
