{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "#from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pandas.api.types\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_data = pd.read_csv('individual_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_id_column_name = \"series_id\"\n",
    "time_column_name = \"time\"\n",
    "event_column_name = \"event\"\n",
    "score_column_name = \"score\"\n",
    "use_scoring_intervals = False #adjust as necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = pd.DataFrame({\n",
    "    series_id_column_name:individual_data[\"series_id\"],\n",
    "    time_column_name: individual_data[\"timestamp\"],\n",
    "    event_column_name: individual_data[\"event\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evd.model import LSTMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "hidden_dim = 64\n",
    "num_layers = 3\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\4019-tjyen\\AppData\\Local\\Temp\\ipykernel_14916\\140715994.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"train_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(4, 64, num_layers=3)\n",
       "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(\"train_model.pth\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('valid_loader.pkl', 'rb') as f:\n",
    "    valid_loader = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evd.utils import calculate_confidence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89728 89728 89728 89728\n"
     ]
    }
   ],
   "source": [
    "#Old version\n",
    "all_series_id = []\n",
    "all_timestamp = []\n",
    "all_confidence_scores = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_idx = 0  # Initial index for batch start\n",
    "    \n",
    "    for X_batch, y_batch in valid_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        batch_size = X_batch.size(0)\n",
    "        \n",
    "        # Extract corresponding slice of series_id and timestamp\n",
    "        series_id_batch = individual_data[\"series_id\"].iloc[start_idx:start_idx + batch_size].values\n",
    "        timestamp_batch = individual_data[\"timestamp\"].iloc[start_idx:start_idx + batch_size].values\n",
    "        \n",
    "        # Process the model outputs\n",
    "        outputs = model(X_batch)\n",
    "        logits = outputs[2]\n",
    "\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        confidence_scores = torch.max(probabilities, dim=1).values\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Append batch data to lists\n",
    "        all_series_id.extend(series_id_batch)\n",
    "        all_timestamp.extend(timestamp_batch)\n",
    "        all_confidence_scores.extend(confidence_scores.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "        # Update start index for next batch\n",
    "        start_idx += batch_size\n",
    "\n",
    "print(len(all_series_id), len(all_timestamp), len(all_confidence_scores), len(all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"series_id\": all_series_id,\n",
    "    \"time\": all_timestamp,\n",
    "    \"event\": all_predictions,\n",
    "    \"score\": all_confidence_scores  \n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      series_id  step_x                 timestamp    anglez      enmo  event  \\\n",
      "0  038441c925bb       0  2018-08-14T15:30:00-0400  0.322257 -0.192628      0   \n",
      "1  038441c925bb       1  2018-08-14T15:30:05-0400  0.322260 -0.194592      0   \n",
      "2  038441c925bb       2  2018-08-14T15:30:10-0400  0.322266 -0.193610      0   \n",
      "3  038441c925bb       3  2018-08-14T15:30:15-0400  0.322260 -0.196556      0   \n",
      "4  038441c925bb       4  2018-08-14T15:30:20-0400  0.322260 -0.194592      0   \n",
      "\n",
      "      _merge  anglez_change  enmo_change     score  \n",
      "0  left_only       0.000000     0.000000  0.999876  \n",
      "1  left_only       0.000003    -0.001964  0.999876  \n",
      "2  left_only       0.000006     0.000982  0.999876  \n",
      "3  left_only      -0.000006    -0.002946  0.999876  \n",
      "4  left_only       0.000000     0.001964  0.999876  \n"
     ]
    }
   ],
   "source": [
    "individual_data = individual_data.merge(\n",
    "    submission[[\"series_id\", \"time\", \"score\"]],  \n",
    "    how=\"left\",\n",
    "    left_on=[\"series_id\", \"timestamp\"], \n",
    "    right_on=[\"series_id\", \"time\"]\n",
    ")\n",
    "\n",
    "individual_data.drop(columns=[\"time\"], inplace=True)\n",
    "\n",
    "\n",
    "print(individual_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_series_id: <class 'list'> 89728\n",
      "all_timestamp: <class 'list'> 89728\n",
      "all_predictions: <class 'list'> 89728\n",
      "all_confidence_scores: <class 'list'> 89728\n"
     ]
    }
   ],
   "source": [
    "print(\"all_series_id:\", type(all_series_id), len(all_series_id))\n",
    "print(\"all_timestamp:\", type(all_timestamp), len(all_timestamp))\n",
    "print(\"all_predictions:\", type(all_predictions), len(all_predictions))\n",
    "print(\"all_confidence_scores:\", type(all_confidence_scores), len(all_confidence_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    8.972800e+04\n",
      "mean     9.998760e-01\n",
      "std      4.359525e-09\n",
      "min      9.998758e-01\n",
      "25%      9.998760e-01\n",
      "50%      9.998760e-01\n",
      "75%      9.998760e-01\n",
      "max      9.998760e-01\n",
      "Name: score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(submission[\"score\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    }
   ],
   "source": [
    "submission[\"time\"] = pd.to_datetime(submission[\"time\"], errors = \"coerce\")\n",
    "\n",
    "submission[\"time\"] = submission[\"time\"].astype(\"int64\") // 10**9\n",
    "\n",
    "print(submission[\"time\"].dtype)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "solution[\"time\"] = pd.to_datetime(solution[\"time\"])\n",
    "solution[\"time\"] = solution[\"time\"].view('int64') / 10**9\n",
    "print(solution[\"time\"].dtype)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def score(\n",
    "    solution: pd.DataFrame,\n",
    "    submission: pd.DataFrame,\n",
    "    tolerances: Dict[str, List[float]],\n",
    "    series_id_column_name: str = \"series_id\",\n",
    "    time_column_name: str = \"time\",\n",
    "    event_column_name: str = \"event\",\n",
    "    score_column_name: str = \"score\",\n",
    "    use_scoring_intervals: bool = False,\n",
    ") -> float:\n",
    "\n",
    "    # Validate metric parameters\n",
    "    assert len(tolerances) > 0, \"Events must have defined tolerances.\"\n",
    "    assert set(tolerances.keys()) == set(solution[event_column_name]).difference({'start', 'end'}),\\\n",
    "        (f\"Solution column {event_column_name} must contain the same events \"\n",
    "         \"as defined in tolerances.\")\n",
    "    assert pd.api.types.is_numeric_dtype(solution[time_column_name]),\\\n",
    "        f\"Solution column {time_column_name} must be of numeric type.\"\n",
    "    \n",
    "    # Validate submission format\n",
    "    for column_name in [\n",
    "        series_id_column_name,\n",
    "        time_column_name,\n",
    "        event_column_name,\n",
    "        score_column_name,\n",
    "    ]:\n",
    "        if column_name not in submission.columns:\n",
    "            raise ParticipantVisibleError(f\"Submission must have column '{column_name}'.\")\n",
    "\n",
    "    if not pd.api.types.is_numeric_dtype(submission[time_column_name]):\n",
    "        raise ParticipantVisibleError(\n",
    "            f\"Submission column '{time_column_name}' must be of numeric type.\"\n",
    "        )\n",
    "    if not pd.api.types.is_numeric_dtype(submission[score_column_name]):\n",
    "        raise ParticipantVisibleError(\n",
    "            f\"Submission column '{score_column_name}' must be of numeric type.\"\n",
    "        )\n",
    "\n",
    "  # Set these globally to avoid passing around a bunch of arguments\n",
    "    globals()['series_id_column_name'] = series_id_column_name\n",
    "    globals()['time_column_name'] = time_column_name\n",
    "    globals()['event_column_name'] = event_column_name\n",
    "    globals()['score_column_name'] = score_column_name\n",
    "    globals()['use_scoring_intervals'] = use_scoring_intervals\n",
    "\n",
    "    return event_detection_ap(solution, submission, tolerances)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerances = {\n",
    "    0: [0.1, 0.5],\n",
    "    1: [0.5, 1.0],\n",
    "    2: [0.5, 1.0],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_detections(\n",
    "        detections: pd.DataFrame, intervals: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Drop detections not inside a scoring interval.\"\"\"\n",
    "    detection_time = detections.loc[:, time_column_name].sort_values().to_numpy()\n",
    "    intervals = intervals.to_numpy()\n",
    "    is_scored = np.full_like(detection_time, False, dtype=bool)\n",
    "\n",
    "    i, j = 0, 0\n",
    "    while i < len(detection_time) and j < len(intervals):\n",
    "        time = detection_time[i]\n",
    "        int_ = intervals[j]\n",
    "\n",
    "        # If the detection is prior in time to the interval, go to the next detection.\n",
    "        if time < int_.left:\n",
    "            i += 1\n",
    "        # If the detection is inside the interval, keep it and go to the next detection.\n",
    "        elif time in int_:\n",
    "            is_scored[i] = True\n",
    "            i += 1\n",
    "        # If the detection is later in time, go to the next interval.\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    return detections.loc[is_scored].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def match_detections(\n",
    "        tolerance: float, ground_truths: pd.DataFrame, detections: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Match detections to ground truth events. Arguments are taken from a common event x tolerance x series_id evaluation group.\"\"\"\n",
    "    detections_sorted = detections.sort_values(score_column_name, ascending=False).dropna()\n",
    "    is_matched = np.full_like(detections_sorted[event_column_name], False, dtype=bool)\n",
    "    gts_matched = set()\n",
    "    for i, det in enumerate(detections_sorted.itertuples(index=False)):\n",
    "        best_error = tolerance\n",
    "        best_gt = None\n",
    "\n",
    "        for gt in ground_truths.itertuples(index=False):\n",
    "            error = abs(getattr(det, time_column_name) - getattr(gt, time_column_name))\n",
    "            if error < best_error and gt not in gts_matched:\n",
    "                best_gt = gt\n",
    "                best_error = error\n",
    "\n",
    "        if best_gt is not None:\n",
    "            is_matched[i] = True\n",
    "            gts_matched.add(best_gt)\n",
    "\n",
    "    detections_sorted['matched'] = is_matched\n",
    "\n",
    "    return detections_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve(\n",
    "        matches: np.ndarray, scores: np.ndarray, p: int\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    if len(matches) == 0:\n",
    "        return [1], [0], []\n",
    "\n",
    "    # Sort matches by decreasing confidence\n",
    "    idxs = np.argsort(scores, kind='stable')[::-1]\n",
    "    scores = scores[idxs]\n",
    "    matches = matches[idxs]\n",
    "\n",
    "    distinct_value_indices = np.where(np.diff(scores))[0]\n",
    "    threshold_idxs = np.r_[distinct_value_indices, matches.size - 1]\n",
    "    thresholds = scores[threshold_idxs]\n",
    "\n",
    "    # Matches become TPs and non-matches FPs as confidence threshold decreases\n",
    "    tps = np.cumsum(matches)[threshold_idxs]\n",
    "    fps = np.cumsum(~matches)[threshold_idxs]\n",
    "\n",
    "    precision = tps / (tps + fps)\n",
    "    precision[np.isnan(precision)] = 0\n",
    "    recall = tps / p  # total number of ground truths might be different than total number of matches\n",
    "\n",
    "    # Stop when full recall attained and reverse the outputs so recall is non-increasing.\n",
    "    last_ind = tps.searchsorted(tps[-1])\n",
    "    sl = slice(last_ind, None, -1)\n",
    "\n",
    "    # Final precision is 1 and final recall is 0\n",
    "    return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]\n",
    "\n",
    "\n",
    "def average_precision_score(matches: np.ndarray, scores: np.ndarray, p: int) -> float:\n",
    "    precision, recall, _ = precision_recall_curve(matches, scores, p)\n",
    "    # Compute step integral\n",
    "    return -np.sum(np.diff(recall) * np.array(precision)[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_detection_ap(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        tolerances: Dict[str, List[float]],\n",
    ") -> float:\n",
    "\n",
    "    # Ensure solution and submission are sorted properly\n",
    "    solution = solution.sort_values([series_id_column_name, time_column_name])\n",
    "    submission = submission.sort_values([series_id_column_name, time_column_name])\n",
    "\n",
    "    # Extract scoring intervals.\n",
    "    if use_scoring_intervals:\n",
    "        intervals = (\n",
    "            solution\n",
    "            .query(\"event in ['start', 'end']\")\n",
    "            .assign(interval=lambda x: x.groupby([series_id_column_name, event_column_name]).cumcount())\n",
    "            .pivot(\n",
    "                index='interval',\n",
    "                columns=[series_id_column_name, event_column_name],\n",
    "                values=time_column_name,\n",
    "            )\n",
    "            .stack(series_id_column_name)\n",
    "            .swaplevel()\n",
    "            .sort_index()\n",
    "            .loc[:, ['start', 'end']]\n",
    "            .apply(lambda x: pd.Interval(*x, closed='both'), axis=1)\n",
    "        )\n",
    "\n",
    "    # Extract ground-truth events.\n",
    "    ground_truths = (\n",
    "        solution\n",
    "        .query(\"event not in ['start', 'end']\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Map each event class to its prevalence (needed for recall calculation)\n",
    "    class_counts = ground_truths.value_counts(event_column_name).to_dict()\n",
    "\n",
    "    # Create table for detections with a column indicating a match to a ground-truth event\n",
    "    detections = submission.assign(matched = False)\n",
    "\n",
    "    # Remove detections outside of scoring intervals\n",
    "    if use_scoring_intervals:\n",
    "        detections_filtered = []\n",
    "        for (det_group, dets), (int_group, ints) in zip(\n",
    "            detections.groupby(series_id_column_name), intervals.groupby(series_id_column_name)\n",
    "        ):\n",
    "            assert det_group == int_group\n",
    "            detections_filtered.append(filter_detections(dets, ints))\n",
    "        detections_filtered = pd.concat(detections_filtered, ignore_index=True)\n",
    "    else:\n",
    "        detections_filtered = detections\n",
    "\n",
    "    # Create table of event-class x tolerance x series_id values\n",
    "    aggregation_keys = pd.DataFrame(\n",
    "        [(ev, tol, vid)\n",
    "         for ev in tolerances.keys()\n",
    "         for tol in tolerances[ev]\n",
    "         for vid in ground_truths[series_id_column_name].unique()],\n",
    "        columns=[event_column_name, 'tolerance', series_id_column_name],\n",
    "    )\n",
    "\n",
    "    # Create match evaluation groups: event-class x tolerance x series_id\n",
    "    detections_grouped = (\n",
    "        aggregation_keys\n",
    "        .merge(detections_filtered, on=[event_column_name, series_id_column_name], how='left')\n",
    "        .groupby([event_column_name, 'tolerance', series_id_column_name])\n",
    "    )\n",
    "    ground_truths_grouped = (\n",
    "        aggregation_keys\n",
    "        .merge(ground_truths, on=[event_column_name, series_id_column_name], how='left')\n",
    "        .groupby([event_column_name, 'tolerance', series_id_column_name])\n",
    "    )\n",
    "    # Match detections to ground truth events by evaluation group\n",
    "    detections_matched = []\n",
    "    for key in aggregation_keys.itertuples(index=False):\n",
    "        dets = detections_grouped.get_group(key)\n",
    "        gts = ground_truths_grouped.get_group(key)\n",
    "        detections_matched.append(\n",
    "            match_detections(dets['tolerance'].iloc[0], gts, dets)\n",
    "        )\n",
    "    detections_matched = pd.concat(detections_matched)\n",
    "\n",
    "    # Compute AP per event x tolerance group\n",
    "    event_classes = ground_truths[event_column_name].unique()\n",
    "    ap_table = (\n",
    "        detections_matched\n",
    "        .query(\"event in @event_classes\")\n",
    "        .groupby([event_column_name, 'tolerance']).apply(\n",
    "            lambda group: average_precision_score(\n",
    "                group['matched'].to_numpy(),\n",
    "                group[score_column_name].to_numpy(),\n",
    "                class_counts[group[event_column_name].iat[0]],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # Average over tolerances, then over event classes\n",
    "    mean_ap = ap_table.groupby(event_column_name).mean().sum() / len(event_classes)\n",
    "\n",
    "    return mean_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch use GPU: NVIDIA GeForce RTX 4080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"PyTorch use GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"PyTorch use CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Move your model to the GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_with_progress(solution, submission, tolerances, **kwargs):\n",
    "    total = len(solution)\n",
    "    for i in tqdm(range(total), desc=\"計算 Mean AP\"):\n",
    "        time.sleep(0.05)  # 模擬運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "計算 Mean AP: 100%|██████████| 389880/389880 [6:31:05<00:00, 16.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AP: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_ap = score_with_progress(\n",
    "    solution=solution,\n",
    "    submission=submission,\n",
    "    tolerances=tolerances\n",
    ")\n",
    "\n",
    "print(f\"Mean AP: {mean_ap}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umap_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
