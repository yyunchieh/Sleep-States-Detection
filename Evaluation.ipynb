{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas.api.types\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evd.model import LSTMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "hidden_dim = 64\n",
    "num_layers = 3\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(\"train_model.pth\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('valid_loader.pkl', 'rb') as f:\n",
    "    valid_loader = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evd.utils import calculate_confidence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_confidence_scores = []\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in valid_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        \n",
    "        #output logits\n",
    "        outputs = model(X_batch)\n",
    "        logits = outputs[2]\n",
    "\n",
    "        #probabilities and confidence score calculation\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        confidence_scores = torch.max(probabilities, dim=1).values\n",
    "\n",
    "        #predict classification\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        #print(confidence_scores,predictions)\n",
    "\n",
    "        all_confidence_scores.extend(confidence_scores.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Set some placeholders for global parameters\n",
    "series_id_column_name = \"series_id\"\n",
    "time_column_name = \"time\"\n",
    "event_column_name = \"event\"\n",
    "#score?\n",
    "score_column_name = \"score\"\n",
    "use_scoring_intervals = None\n",
    "\n",
    "\n",
    "def score(\n",
    "        solution: pd.individual_data,\n",
    "        #submussion dataframe?\n",
    "        submission: pd.DataFrame,\n",
    "        tolerances: Dict[str, List[float]],\n",
    "        series_id_column_name: \"series_id\",\n",
    "        time_column_name: \"time\",\n",
    "        event_column_name: \"event\",\n",
    "        score_column_name: \"score\",\n",
    "        use_scoring_intervals: bool = False,\n",
    ") -> float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Validate metric parameters\n",
    "    assert len(tolerances) > 0, \"Events must have defined tolerances.\"\n",
    "    assert set(tolerances.keys()) == set(solution[event_column_name]).difference({'start', 'end'}),\\\n",
    "        (f\"Solution column {event_column_name} must contain the same events \"\n",
    "         \"as defined in tolerances.\")\n",
    "    assert pd.api.types.is_numeric_dtype(solution[time_column_name]),\\\n",
    "        f\"Solution column {time_column_name} must be of numeric type.\"\n",
    "\n",
    "    # Validate submission format\n",
    "    for column_name in [\n",
    "        series_id_column_name,\n",
    "        time_column_name,\n",
    "        event_column_name,\n",
    "        score_column_name,\n",
    "    ]:\n",
    "        if column_name not in submission.columns:\n",
    "            raise ParticipantVisibleError(f\"Submission must have column '{target_name}'.\")\n",
    "\n",
    "    if not pd.api.types.is_numeric_dtype(submission[time_column_name]):\n",
    "        raise ParticipantVisibleError(\n",
    "            f\"Submission column '{time_column_name}' must be of numeric type.\"\n",
    "        )\n",
    "    if not pd.api.types.is_numeric_dtype(submission[score_column_name]):\n",
    "        raise ParticipantVisibleError(\n",
    "            f\"Submission column '{score_column_name}' must be of numeric type.\"\n",
    "        )\n",
    "\n",
    "# Set these globally to avoid passing around a bunch of arguments\n",
    "globals()['series_id_column_name'] = series_id_column_name\n",
    "globals()['time_column_name'] = time_column_name\n",
    "globals()['event_column_name'] = event_column_name\n",
    "globals()['score_column_name'] = score_column_name\n",
    "globals()['use_scoring_intervals'] = use_scoring_intervals\n",
    "\n",
    "    return event_detection_ap(solution, submission, tolerances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_detections(\n",
    "        detections: pd.DataFrame, intervals: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Drop detections not inside a scoring interval.\"\"\"\n",
    "    detection_time = detections.loc[:, time_column_name].sort_values().to_numpy()\n",
    "    intervals = intervals.to_numpy()\n",
    "    is_scored = np.full_like(detection_time, False, dtype=bool)\n",
    "\n",
    "    i, j = 0, 0\n",
    "    while i < len(detection_time) and j < len(intervals):\n",
    "        time = detection_time[i]\n",
    "        int_ = intervals[j]\n",
    "\n",
    "        # If the detection is prior in time to the interval, go to the next detection.\n",
    "        if time < int_.left:\n",
    "            i += 1\n",
    "        # If the detection is inside the interval, keep it and go to the next detection.\n",
    "        elif time in int_:\n",
    "            is_scored[i] = True\n",
    "            i += 1\n",
    "        # If the detection is later in time, go to the next interval.\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    return detections.loc[is_scored].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def match_detections(\n",
    "        tolerance: float, ground_truths: pd.DataFrame, detections: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Match detections to ground truth events. Arguments are taken from a common event x tolerance x series_id evaluation group.\"\"\"\n",
    "    detections_sorted = detections.sort_values(score_column_name, ascending=False).dropna()\n",
    "    is_matched = np.full_like(detections_sorted[event_column_name], False, dtype=bool)\n",
    "    gts_matched = set()\n",
    "    for i, det in enumerate(detections_sorted.itertuples(index=False)):\n",
    "        best_error = tolerance\n",
    "        best_gt = None\n",
    "\n",
    "        for gt in ground_truths.itertuples(index=False):\n",
    "            error = abs(getattr(det, time_column_name) - getattr(gt, time_column_name))\n",
    "            if error < best_error and gt not in gts_matched:\n",
    "                best_gt = gt\n",
    "                best_error = error\n",
    "\n",
    "        if best_gt is not None:\n",
    "            is_matched[i] = True\n",
    "            gts_matched.add(best_gt)\n",
    "\n",
    "    detections_sorted['matched'] = is_matched\n",
    "\n",
    "    return detections_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve(\n",
    "        matches: np.ndarray, scores: np.ndarray, p: int\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    if len(matches) == 0:\n",
    "        return [1], [0], []\n",
    "\n",
    "    # Sort matches by decreasing confidence\n",
    "    idxs = np.argsort(scores, kind='stable')[::-1]\n",
    "    scores = scores[idxs]\n",
    "    matches = matches[idxs]\n",
    "\n",
    "    distinct_value_indices = np.where(np.diff(scores))[0]\n",
    "    threshold_idxs = np.r_[distinct_value_indices, matches.size - 1]\n",
    "    thresholds = scores[threshold_idxs]\n",
    "\n",
    "    # Matches become TPs and non-matches FPs as confidence threshold decreases\n",
    "    tps = np.cumsum(matches)[threshold_idxs]\n",
    "    fps = np.cumsum(~matches)[threshold_idxs]\n",
    "\n",
    "    precision = tps / (tps + fps)\n",
    "    precision[np.isnan(precision)] = 0\n",
    "    recall = tps / p  # total number of ground truths might be different than total number of matches\n",
    "\n",
    "    # Stop when full recall attained and reverse the outputs so recall is non-increasing.\n",
    "    last_ind = tps.searchsorted(tps[-1])\n",
    "    sl = slice(last_ind, None, -1)\n",
    "\n",
    "    # Final precision is 1 and final recall is 0\n",
    "    return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]\n",
    "\n",
    "\n",
    "def average_precision_score(matches: np.ndarray, scores: np.ndarray, p: int) -> float:\n",
    "    precision, recall, _ = precision_recall_curve(matches, scores, p)\n",
    "    # Compute step integral\n",
    "    return -np.sum(np.diff(recall) * np.array(precision)[:-1])\n",
    "\n",
    "\n",
    "def event_detection_ap(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        tolerances: Dict[str, List[float]],\n",
    ") -> float:\n",
    "\n",
    "    # Ensure solution and submission are sorted properly\n",
    "    solution = solution.sort_values([series_id_column_name, time_column_name])\n",
    "    submission = submission.sort_values([series_id_column_name, time_column_name])\n",
    "\n",
    "    # Extract scoring intervals.\n",
    "    if use_scoring_intervals:\n",
    "        intervals = (\n",
    "            solution\n",
    "            .query(\"event in ['start', 'end']\")\n",
    "            .assign(interval=lambda x: x.groupby([series_id_column_name, event_column_name]).cumcount())\n",
    "            .pivot(\n",
    "                index='interval',\n",
    "                columns=[series_id_column_name, event_column_name],\n",
    "                values=time_column_name,\n",
    "            )\n",
    "            .stack(series_id_column_name)\n",
    "            .swaplevel()\n",
    "            .sort_index()\n",
    "            .loc[:, ['start', 'end']]\n",
    "            .apply(lambda x: pd.Interval(*x, closed='both'), axis=1)\n",
    "        )\n",
    "\n",
    "    # Extract ground-truth events.\n",
    "    ground_truths = (\n",
    "        solution\n",
    "        .query(\"event not in ['start', 'end']\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Map each event class to its prevalence (needed for recall calculation)\n",
    "    class_counts = ground_truths.value_counts(event_column_name).to_dict()\n",
    "\n",
    "    # Create table for detections with a column indicating a match to a ground-truth event\n",
    "    detections = submission.assign(matched = False)\n",
    "\n",
    "    # Remove detections outside of scoring intervals\n",
    "    if use_scoring_intervals:\n",
    "        detections_filtered = []\n",
    "        for (det_group, dets), (int_group, ints) in zip(\n",
    "            detections.groupby(series_id_column_name), intervals.groupby(series_id_column_name)\n",
    "        ):\n",
    "            assert det_group == int_group\n",
    "            detections_filtered.append(filter_detections(dets, ints))\n",
    "        detections_filtered = pd.concat(detections_filtered, ignore_index=True)\n",
    "    else:\n",
    "        detections_filtered = detections\n",
    "\n",
    "    # Create table of event-class x tolerance x series_id values\n",
    "    aggregation_keys = pd.DataFrame(\n",
    "        [(ev, tol, vid)\n",
    "         for ev in tolerances.keys()\n",
    "         for tol in tolerances[ev]\n",
    "         for vid in ground_truths[series_id_column_name].unique()],\n",
    "        columns=[event_column_name, 'tolerance', series_id_column_name],\n",
    "    )\n",
    "\n",
    "    # Create match evaluation groups: event-class x tolerance x series_id\n",
    "    detections_grouped = (\n",
    "        aggregation_keys\n",
    "        .merge(detections_filtered, on=[event_column_name, series_id_column_name], how='left')\n",
    "        .groupby([event_column_name, 'tolerance', series_id_column_name])\n",
    "    )\n",
    "    ground_truths_grouped = (\n",
    "        aggregation_keys\n",
    "        .merge(ground_truths, on=[event_column_name, series_id_column_name], how='left')\n",
    "        .groupby([event_column_name, 'tolerance', series_id_column_name])\n",
    "    )\n",
    "    # Match detections to ground truth events by evaluation group\n",
    "    detections_matched = []\n",
    "    for key in aggregation_keys.itertuples(index=False):\n",
    "        dets = detections_grouped.get_group(key)\n",
    "        gts = ground_truths_grouped.get_group(key)\n",
    "        detections_matched.append(\n",
    "            match_detections(dets['tolerance'].iloc[0], gts, dets)\n",
    "        )\n",
    "    detections_matched = pd.concat(detections_matched)\n",
    "\n",
    "    # Compute AP per event x tolerance group\n",
    "    event_classes = ground_truths[event_column_name].unique()\n",
    "    ap_table = (\n",
    "        detections_matched\n",
    "        .query(\"event in @event_classes\")\n",
    "        .groupby([event_column_name, 'tolerance']).apply(\n",
    "            lambda group: average_precision_score(\n",
    "                group['matched'].to_numpy(),\n",
    "                group[score_column_name].to_numpy(),\n",
    "                class_counts[group[event_column_name].iat[0]],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # Average over tolerances, then over event classes\n",
    "    mean_ap = ap_table.groupby(event_column_name).mean().sum() / len(event_classes)\n",
    "\n",
    "    return mean_ap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umap_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
